name: "net_d"
layer {
  name: "data"
# type: "MemoryData"
  type: "Input"
  top: "data"
  top: "label"
  input_param {
  	shape: {
  		dim: 1
  		dim: 3
  		dim: 64
  		dim: 64
  	}
  }
#  memory_data_param 
#  {
#    batch_size: 64
#    channels: 3
#    height: 64
#    width: 64 
#  }
}

#########################################################################################
# Torch (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 4
    stride: 2
  }
}

#########################################################################################
# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.2
  }
}

#########################################################################################
# Torch (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 4
    stride: 2
  }
}

# Torch (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
layer {
  name: "norm1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "norm1"
  batch_norm_param {
    eps: 1e-05
  }
}

# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "norm1"
  top: "norm1"
  relu_param {
    negative_slope: 0.2
  }
}

#########################################################################################
# Torch (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 4
    stride: 2
  }
}

# Torch (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
layer {
  name: "norm2"
  type: "BatchNorm"
  bottom: "conv3"
  top: "norm2"
  batch_norm_param {
    eps: 1e-05
  }
}

# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "norm2"
  top: "norm2"
  relu_param {
    negative_slope: 0.2
  }
}

#########################################################################################
# Torch (pyramid.256-512.conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm2"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 4
    stride: 2
  }
}

# Torch (pyramid.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
layer {
  name: "norm3"
  type: "BatchNorm"
  bottom: "conv4"
  top: "norm3"
  batch_norm_param {
    eps: 1e-05
  }
}

# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "norm3"
  top: "norm3"
  relu_param {
    negative_slope: 0.2
  }
}

#########################################################################################
# Torch (final.512-1.conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "norm3"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 1
    pad: 0
    kernel_size: 4
    stride: 1
  }
}

