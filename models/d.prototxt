
name: "net_d"

force_backward: true

input: "data"
input_shape {
  dim: 64
  dim: 3
  dim: 64
  dim: 64
}

input: "label"
input_shape {
  dim: 64
  dim: 1
  dim: 1
  dim: 1
}

################################################################################
# Torch (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "gaussian"
      mean: 0.0
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

################################################################################
# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.2
  }
}

################################################################################
# Torch (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "gaussian"
      mean: 0.0
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

# Torch (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
layer {
  name: "norm1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "norm1"
  batch_norm_param {
    eps: 1e-05
  }
}

# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "norm1"
  top: "norm1"
  relu_param {
    negative_slope: 0.2
  }
}

################################################################################
# Torch (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "gaussian"
      mean: 0.0
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

# Torch (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
layer {
  name: "norm2"
  type: "BatchNorm"
  bottom: "conv3"
  top: "norm2"
  batch_norm_param {
    eps: 1e-05
  }
}

# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "norm2"
  top: "norm2"
  relu_param {
    negative_slope: 0.2
  }
}

################################################################################
# Torch (pyramid.256-512.conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm2"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "gaussian"
      mean: 0.0
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

# Torch (pyramid.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
layer {
  name: "norm3"
  type: "BatchNorm"
  bottom: "conv4"
  top: "norm3"
  batch_norm_param {
    eps: 1e-05
  }
}

# Torch LeakyReLU (0.2, inplace)
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "norm3"
  top: "norm3"
  relu_param {
    negative_slope: 0.2
  }
}

################################################################################
# Torch (final.512-1.conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "norm3"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 1
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0.0
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "Dfc7"
  type: "InnerProduct"
  bottom: "conv5"
  top: "Dfc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

################################################################################
layer {
  name: "loss"
  type: "SigmoidCrossEntropyLoss"
  bottom: "Dfc7"
  bottom: "label"
  top: "loss"
  loss_weight: 1
}
